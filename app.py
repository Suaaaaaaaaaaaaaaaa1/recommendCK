import streamlit as st
import pandas as pd
import pickle
import os
import ast
import gdown
import random 
import re  # <-- THÃŠM THÆ¯ VIá»†N NÃ€Y
from surprise import SVD

# --- 1. Äá»‹nh nghÄ©a TÃªn tá»‡p vÃ  File IDs ---
MODEL_FILE_PATH = 'svd_model.pkl' 
METADATA_FILE_PATH = 'recipes_metadata.csv' 

# !!! ID Cá»¦A Báº N Tá»ª Láº¦N TRÆ¯á»šC !!!
MODEL_FILE_ID = '1mSWLAjm2Ho6Aox61PrIQJUgNyJObKSbu' 
METADATA_FILE_ID = '1jCm7OruZnwkkd5GRU42dycNcQdGOKNRv'

# --- 2. HÃ m Táº£i tá»‡p chung ---
def download_file_from_gdrive(file_id, dest_path):
    if not os.path.exists(dest_path):
        with st.spinner(f"Äang táº£i tÃ i nguyÃªn: {dest_path} (láº§n Ä‘áº§u tiÃªn)..."):
            url = f'https://drive.google.com/uc?id={file_id}'
            gdown.download(url, dest_path, quiet=False)
    return dest_path

# --- 3. HÃ m Táº£i Model vÃ  Dá»¯ liá»‡u ---
@st.cache_resource
def load_model(file_id, dest_path):
    try:
        model_path = download_file_from_gdrive(file_id, dest_path)
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        return model
    except Exception as e:
        st.error(f"Lá»–I khi táº£i mÃ´ hÃ¬nh: {e}")
        return None

@st.cache_data
def load_metadata(file_id, dest_path):
    try:
        metadata_path = download_file_from_gdrive(file_id, dest_path)
        df = pd.read_csv(metadata_path)
        return df
    except Exception as e:
        st.error(f"Lá»–I khi táº£i metadata: {e}")
        return pd.DataFrame()

# --- 4. HÃ€M Láº¤Y HÃŒNH áº¢NH (ÄÃƒ Sá»¬A Lá»–I HOÃ€N TOÃ€N) ---
def get_first_image_url(images_str):
    placeholder_image = "https://cdn.freelogovectors.net/wp-content/uploads/2022/10/foodcom-logo-freelogovectors.net_-400x144.png"
    
    # Kiá»ƒm tra náº¿u dá»¯ liá»‡u rá»—ng hoáº·c khÃ´ng pháº£i chuá»—i
    if not isinstance(images_str, str) or pd.isna(images_str):
        return placeholder_image

    # DÃ¹ng regex Ä‘á»ƒ tÃ¬m URL Ä‘áº§u tiÃªn (bÃªn trong dáº¥u " ")
    # (Há»— trá»£ cáº£ Ä‘á»‹nh dáº¡ng c("url1",...) vÃ  ['url1',...])
    match = re.search(r'"(https://[^"]+)"', images_str)
    
    if match:
        # Náº¿u tÃ¬m tháº¥y (vÃ­ dá»¥: c("url1", ...))
        return match.group(1) # match.group(1) lÃ  url1
    else:
        # Náº¿u khÃ´ng tÃ¬m tháº¥y (vÃ­ dá»¥: URL tráº§n khÃ´ng cÃ³ dáº¥u ")
        if images_str.startswith('http'):
            return images_str
            
    # Náº¿u má»i thá»© tháº¥t báº¡i (vÃ­ dá»¥: [] hoáº·c "")
    return placeholder_image

# --- 5. HÃ€M TÃNH TOÃN (ÄÃ£ bá» sample) ---
def get_all_predictions(user_id):
    all_ids = all_recipe_ids_tuple
    predictions = []
    for recipe_id in all_ids:
        pred = model.predict(uid=user_id, iid=recipe_id)
        predictions.append((recipe_id, pred.est))
        
    predictions.sort(key=lambda x: x[1], reverse=True)
    return predictions

# --- 6. XÃ¢y dá»±ng giao diá»‡n Streamlit ---
st.set_page_config(layout="wide")
st.title("Há»‡ thá»‘ng Gá»£i Ã½ MÃ³n Äƒn ðŸ² ðŸ³ ðŸ°")

model = load_model(MODEL_FILE_ID, MODEL_FILE_PATH)
metadata_df = load_metadata(METADATA_FILE_ID, METADATA_FILE_PATH)

if model and not metadata_df.empty:
    st.header("TÃ¬m mÃ³n Äƒn cho báº¡n")
    
    # Sá»­a lá»—i typo (gÃµ nháº§m)
    all_recipe_ids_tuple = tuple(metadata_df['RecipeId'].unique())
    metadata_df = metadata_df.set_index('RecipeId')
    
    # --- WIDGETS ---
    user_id_input = st.number_input(
        "Nháº­p User ID cá»§a báº¡n:", 
        min_value=1, 
        value=1535,
        step=1,
        help="HÃ£y nháº­p má»™t User ID (vÃ­ dá»¥: 1535, 2046, 5201...)"
    )
    
    num_recs = st.slider("Sá»‘ lÆ°á»£ng gá»£i Ã½:", min_value=5, max_value=20, value=10)

    # --- THÃŠM Láº I NÃšT Báº¤M ---
    if st.button("TÃ¬m kiáº¿m gá»£i Ã½"):
        with st.spinner("Äang tÃ­nh toÃ¡n gá»£i Ã½ (trÃªn toÃ n bá»™ dá»¯ liá»‡u)..."):
            all_preds = get_all_predictions(user_id_input) 
            st.session_state['all_predictions'] = all_preds
    
    # --- HIá»‚N THá»Š Káº¾T QUáº¢ Tá»ª SESSION_STATE ---
    if 'all_predictions' in st.session_state:
        all_preds = st.session_state['all_predictions']
        top_n_preds = all_preds[:num_recs]
        
        top_n_ids = [recipe_id for recipe_id, score in top_n_preds]
        
        valid_top_n_ids = [idx for idx in top_n_ids if idx in metadata_df.index]
        if valid_top_n_ids:
            recs_df = metadata_df.loc[valid_top_n_ids].copy()
            
            st.subheader(f"Káº¿t quáº£ gá»£i Ã½:")
            
            cols = st.columns(2)
            col_idx = 0
            
            for index, row in recs_df.iterrows():
                with cols[col_idx]:
                    # DÃ™NG HÃ€M Má»šI (ÄÃƒ Sá»¬A Lá»–I)
                    image_url = get_first_image_url(row['Images'])
                    
                    st.image(image_url, caption=f"Recipe ID: {row.name}", use_container_width=True)
                    st.subheader(row['Name'])
                    if 'Description' in row and pd.notna(row['Description']):
                         st.markdown(f"**MÃ´ táº£:** {row['Description'][:150]}...")
                    st.divider()
                
                col_idx = (col_idx + 1) % 2
        else:
            st.warning("KhÃ´ng tÃ¬m tháº¥y gá»£i Ã½ nÃ o.")
else:
    st.error("KhÃ´ng thá»ƒ táº£i mÃ´ hÃ¬nh hoáº·c dá»¯ liá»‡u tá»« Google Drive. Vui lÃ²ng kiá»ƒm tra láº¡i File IDs.")
