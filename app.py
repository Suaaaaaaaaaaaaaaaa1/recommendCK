
import streamlit as st
import pandas as pd
import pickle
import os
import ast
import gdown
from surprise import SVD

# --- 1. ƒê·ªãnh nghƒ©a T√™n t·ªáp v√† File IDs ---
MODEL_FILE_PATH = 'svd_model.pkl'
METADATA_FILE_PATH = 'recipes_metadata.csv'

# !!! THAY TH·∫æ C√ÅC ID C·ª¶A B·∫†N V√ÄO ƒê√ÇY !!!
MODEL_FILE_ID = '16v3zUzOhPqnF6n3-80lYq7UcYRmej7RJ' 
METADATA_FILE_ID = '1x_Zb0mO_rOjhep71QveJcVleBLO2HCEs'

# --- 2. H√†m T·∫£i t·ªáp chung ---
def download_file_from_gdrive(file_id, dest_path):
    if not os.path.exists(dest_path):
        with st.spinner(f"ƒêang t·∫£i t√†i nguy√™n: {dest_path} (l·∫ßn ƒë·∫ßu ti√™n)..."):
            url = f'https://drive.google.com/uc?id={file_id}'
            gdown.download(url, dest_path, quiet=False)
    return dest_path

# --- 3. H√†m T·∫£i Model v√† D·ªØ li·ªáu ---
@st.cache_resource
def load_model(file_id, dest_path):
    try:
        model_path = download_file_from_gdrive(file_id, dest_path)
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        return model
    except Exception as e:
        st.error(f"L·ªñI khi t·∫£i m√¥ h√¨nh: {e}")
        return None

@st.cache_data
def load_metadata(file_id, dest_path):
    try:
        metadata_path = download_file_from_gdrive(file_id, dest_path)
        df = pd.read_csv(metadata_path)
        # B√ÇY GI·ªú KH√îNG set_index v·ªôi
        return df
    except Exception as e:
        st.error(f"L·ªñI khi t·∫£i metadata: {e}")
        return pd.DataFrame()

# --- 4. H√†m l·∫•y h√¨nh ·∫£nh (Kh√¥ng ƒë·ªïi) ---
def get_first_image_url(images_str):
    placeholder_image = "https://i.imgur.com/gY9R3t1.png" 
    if not isinstance(images_str, str) or pd.isna(images_str):
        return placeholder_image
    try:
        evaluated_data = ast.literal_eval(images_str)
        if isinstance(evaluated_data, list):
            if len(evaluated_data) > 0:
                return evaluated_data[0]
            else:
                return placeholder_image
        if isinstance(evaluated_data, str):
            if evaluated_data.startswith('http'):
                return evaluated_data
            else:
                return placeholder_image
    except (ValueError, SyntaxError):
        if images_str.startswith('http'):
            return images_str
    return placeholder_image

# --- 5. H√ÄM T√çNH TO√ÅN (C√ì CACHE) ---
# ƒê√¢y l√† ph·∫ßn quan tr·ªçng nh·∫•t
# @_st.cache_data b·∫£o Streamlit l∆∞u k·∫øt qu·∫£ v√†o b·ªô ƒë·ªám
# N√≥ s·∫Ω ch·ªâ ch·∫°y l·∫°i h√†m n√†y khi user_id ho·∫∑c all_recipe_ids thay ƒë·ªïi
@st.cache_data
def get_all_predictions(user_id, _model, all_recipe_ids):
    """
    T√≠nh to√°n v√† s·∫Øp x·∫øp T·∫§T C·∫¢ d·ª± ƒëo√°n cho m·ªôt user.
    H√†m n√†y ƒë∆∞·ª£c cache ƒë·ªÉ ch·∫°y nhanh.
    """
    predictions = []
    for recipe_id in all_recipe_ids:
        # _model l√† m√¥ h√¨nh SVD
        pred = _model.predict(uid=user_id, iid=recipe_id)
        predictions.append((recipe_id, pred.est))
        
    # S·∫Øp x·∫øp 1 l·∫ßn duy nh·∫•t
    predictions.sort(key=lambda x: x[1], reverse=True)
    return predictions

# --- 6. X√¢y d·ª±ng giao di·ªán Streamlit ---
st.set_page_config(layout="wide")
st.title("H·ªá th·ªëng G·ª£i √Ω M√≥n ƒÉn üç≤ üç≥ üç∞")

# T·∫£i model v√† metadata
model = load_model(MODEL_FILE_ID, MODEL_FILE_PATH)
metadata_df = load_metadata(METADATA_FILE_ID, METADATA_FILE_PATH)

if model and not metadata_df.empty:
    st.header("T√¨m m√≥n ƒÉn cho b·∫°n")
    
    # L·∫•y danh s√°ch ID m√≥n ƒÉn (ch·ªâ ch·∫°y 1 l·∫ßn)
    all_recipe_ids_list = metadata_df['Recipe_ID'].unique()
    
    # ƒê·∫∑t index sau khi ƒë√£ l·∫•y list ·ªü tr√™n (ƒë·ªÉ tra c·ª©u nhanh)
    metadata_df = metadata_df.set_index('Recipe_ID')
    
    user_id_input = st.number_input(
        "Nh·∫≠p User ID c·ªßa b·∫°n:", 
        min_value=1, 
        value=1535,
        step=1,
        help="H√£y nh·∫≠p m·ªôt User ID (v√≠ d·ª•: 1535, 2046, 5201...)"
    )
    
    num_recs = st.slider("S·ªë l∆∞·ª£ng g·ª£i √Ω:", min_value=5, max_value=20, value=10)

    # --- S·ª¨A L·ªñI T·∫†I ƒê√ÇY ---
    with st.spinner("ƒêang t√≠nh to√°n g·ª£i √Ω..."):
        
        # 1. G·ªçi h√†m CACHE (ch·∫°y r·∫•t nhanh n·∫øu user_id kh√¥ng ƒë·ªïi)
        all_preds = get_all_predictions(user_id_input, model, all_recipe_ids_list)
        
        # 2. L·∫•y Top N (ch·ªâ l√† 1 thao t√°c slice, si√™u nhanh)
        top_n_preds = all_preds[:num_recs]
        
        # 3. L·∫•y Recipe IDs t·ª´ k·∫øt qu·∫£ slice
        top_n_ids = [recipe_id for recipe_id, score in top_n_preds]
        
        # 4. Tra c·ª©u metadata (ch·ªâ tra c·ª©u N m√≥n, kh√¥ng ph·∫£i 500k)
        recs_df = metadata_df.loc[top_n_ids].copy()
        
        st.subheader(f"G·ª£i √Ω cho User {user_id_input}:")
        
        cols = st.columns(2)
        col_idx = 0
        
        for index, row in recs_df.iterrows():
            with cols[col_idx]:
                # D√πng h√†m ƒë√£ s·ª≠a l·ªói ·∫£nh
                image_url = get_first_image_url(row['Images'])
                st.image(image_url, caption=f"Recipe ID: {row.name}", use_column_width=True)
                st.subheader(row['Name'])
                if 'Description' in row and pd.notna(row['Description']):
                     st.markdown(f"**M√¥ t·∫£:** {row['Description'][:150]}...")
                st.divider()
            
            col_idx = (col_idx + 1) % 2
else:
    st.error("Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh ho·∫∑c d·ªØ li·ªáu t·ª´ Google Drive. Vui l√≤ng ki·ªÉm tra l·∫°i File IDs.")
