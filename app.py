import streamlit as st
import pandas as pd
import pickle
import os
import ast
import gdown
import random # <-- TH√äM TH∆Ø VI·ªÜN N√ÄY
from surprise import SVD

# --- 1. ƒê·ªãnh nghƒ©a T√™n t·ªáp v√† File IDs ---
MODEL_FILE_PATH = 'svd_model.pkl' 
METADATA_FILE_PATH = 'recipes_metadata.csv' 

# !!! THAY TH·∫æ C√ÅC ID C·ª¶A B·∫†N V√ÄO ƒê√ÇY !!!
MODEL_FILE_ID = '16v3zUzOhPqnF6n3-80lYq7UcYRmej7RJ' 
METADATA_FILE_ID = '1x_Zb0mO_rOjhep71QveJcVleBLO2HCEs'

# --- 2. H√†m T·∫£i t·ªáp chung ---
def download_file_from_gdrive(file_id, dest_path):
    if not os.path.exists(dest_path):
        with st.spinner(f"ƒêang t·∫£i t√†i nguy√™n: {dest_path} (l·∫ßn ƒë·∫ßu ti√™n)..."):
            url = f'https://drive.google.com/uc?id={file_id}'
            gdown.download(url, dest_path, quiet=False)
    return dest_path

# --- 3. H√†m T·∫£i Model v√† D·ªØ li·ªáu ---
@st.cache_resource
def load_model(file_id, dest_path):
    try:
        model_path = download_file_from_gdrive(file_id, dest_path)
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        return model
    except Exception as e:
        st.error(f"L·ªñI khi t·∫£i m√¥ h√¨nh: {e}")
        return None

@st.cache_data
def load_metadata(file_id, dest_path):
    try:
        metadata_path = download_file_from_gdrive(file_id, dest_path)
        df = pd.read_csv(metadata_path)
        return df
    except Exception as e:
        st.error(f"L·ªñI khi t·∫£i metadata: {e}")
        return pd.DataFrame()

# --- 4. H√†m l·∫•y h√¨nh ·∫£nh (ƒê√É S·ª¨A L·ªñI) ---
def get_first_image_url(images_str):
    # <<< S·ª¨A L·ªñI 1 T·∫†I ƒê√ÇY: D√πng URL l√†m placeholder
    placeholder_image = "https://i.imgur.com/gY9R3t1.png" 
    
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p d·ªØ li·ªáu b·ªã thi·∫øu (NaN)
    if not isinstance(images_str, str) or pd.isna(images_str):
        return placeholder_image

    try:
        # Th·ª≠ chuy·ªÉn ƒë·ªïi chu·ªói (v√≠ d·ª•: "['url1']" ho·∫∑c "'url1'")
        evaluated_data = ast.literal_eval(images_str)

        # Case 1: N·∫øu k·∫øt qu·∫£ l√† m·ªôt LIST (v√≠ d·ª•: ['url1', 'url2'])
        if isinstance(evaluated_data, list):
            if len(evaluated_data) > 0:
                return evaluated_data[0] # L·∫•y ·∫£nh ƒë·∫ßu ti√™n
            else:
                return placeholder_image # List r·ªóng []

        # Case 2: N·∫øu k·∫øt qu·∫£ l√† m·ªôt STRING (v√≠ d·ª•: 'url1')
        if isinstance(evaluated_data, str):
            if evaluated_data.startswith('http'):
                return evaluated_data # Tr·∫£ v·ªÅ ch√≠nh chu·ªói ƒë√≥
            else:
                return placeholder_image # Chu·ªói r·ªóng ""

    except (ValueError, SyntaxError):
        # Case 3: N·∫øu kh√¥ng ph·∫£i ƒë·ªãnh d·∫°ng chu·∫©n (v√≠ d·ª•: ch·ªâ l√† http... kh√¥ng c√≥ d·∫•u nh√°y)
        if images_str.startswith('http'):
            return images_str
    
    # N·∫øu th·∫•t b·∫°i ·ªü m·ªçi tr∆∞·ªùng h·ª£p
    return placeholder_image

# --- 5. H√ÄM T√çNH TO√ÅN (ƒê√£ t·ªëi ∆∞u h√≥a) ---
@st.cache_data
def get_sampled_predictions(user_id, _model, all_recipe_ids, sample_size=20000):
    """
    T√≠nh to√°n d·ª± ƒëo√°n tr√™n m·ªôt M·∫™U NG·∫™U NHI√äN ƒë·ªÉ tr√°nh crash RAM.
    """
    if len(all_recipe_ids) > sample_size:
        sampled_ids = random.sample(list(all_recipe_ids), sample_size)
    else:
        sampled_ids = all_recipe_ids

    predictions = []
    for recipe_id in sampled_ids:
        pred = _model.predict(uid=user_id, iid=recipe_id)
        predictions.append((recipe_id, pred.est))
        
    predictions.sort(key=lambda x: x[1], reverse=True)
    return predictions

# --- 6. X√¢y d·ª±ng giao di·ªán Streamlit ---
st.set_page_config(layout="wide")
st.title("H·ªá th·ªëng G·ª£i √Ω M√≥n ƒÉn üç≤ üç≥ üç∞")

model = load_model(MODEL_FILE_ID, MODEL_FILE_PATH)
metadata_df = load_metadata(METADATA_FILE_ID, METADATA_FILE_PATH)

if model and not metadata_df.empty:
    st.header("T√¨m m√≥n ƒÉn cho b·∫°n")
    
    all_recipe_ids_list = metadata_df['RecipeId'].unique()
    metadata_df = metadata_df.set_index('RecipeId')
    
    user_id_input = st.number_input(
        "Nh·∫≠p User ID c·ªßa b·∫°n:", 
        min_value=1, 
        value=1535,
        step=1,
        help="H√£y nh·∫≠p m·ªôt User ID (v√≠ d·ª•: 1535, 2046, 5201...)"
    )
    
    num_recs = st.slider("S·ªë l∆∞·ª£ng g·ª£i √Ω:", min_value=5, max_value=20, value=10)

    with st.spinner("ƒêang t√≠nh to√°n g·ª£i √Ω..."):
        
        all_preds = get_sampled_predictions(user_id_input, model, all_recipe_ids_list)
        top_n_preds = all_preds[:num_recs]
        top_n_ids = [recipe_id for recipe_id, score in top_n_preds]
        
        # Ki·ªÉm tra n·∫øu top_n_ids kh√¥ng r·ªóng
        if top_n_ids:
            recs_df = metadata_df.loc[top_n_ids].copy()
            
            st.subheader(f"G·ª£i √Ω cho User {user_id_input}:")
            
            cols = st.columns(2)
            col_idx = 0
            
            for index, row in recs_df.iterrows():
                with cols[col_idx]:
                    image_url = get_first_image_url(row['Images'])
                    
                    # <<< S·ª¨A L·ªñI 2 T·∫†I ƒê√ÇY: ƒê·ªïi sang use_container_width
                    st.image(image_url, caption=f"Recipe ID: {row.name}", use_container_width=True)
                    
                    st.subheader(row['Name'])
                    if 'Description' in row and pd.notna(row['Description']):
                         st.markdown(f"**M√¥ t·∫£:** {row['Description'][:150]}...")
                    st.divider()
                
                col_idx = (col_idx + 1) % 2
        else:
            st.warning("Kh√¥ng t√¨m th·∫•y g·ª£i √Ω n√†o. (C√≥ th·ªÉ do l·ªói l·∫•y m·∫´u)")
else:
    st.error("Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh ho·∫∑c d·ªØ li·ªáu t·ª´ Google Drive. Vui l√≤ng ki·ªÉm tra l·∫°i File IDs.")
